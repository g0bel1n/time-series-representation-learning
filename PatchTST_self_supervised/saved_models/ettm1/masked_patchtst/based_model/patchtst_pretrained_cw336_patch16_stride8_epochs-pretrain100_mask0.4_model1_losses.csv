train_loss,valid_loss
0.996895,0.962190
0.975972,0.959705
0.971651,0.958695
0.970053,0.958806
0.969042,0.958279
0.968657,0.958514
0.967625,0.956734
0.965889,0.940857
0.909350,0.829292
0.802673,0.640420
0.691537,0.560720
0.645908,0.505665
0.537932,0.320129
0.428455,0.250771
0.357835,0.221743
0.315640,0.201593
0.284909,0.184618
0.261445,0.168243
0.242627,0.160899
0.228501,0.152562
0.218043,0.146981
0.209656,0.140631
0.201365,0.133984
0.193884,0.128670
0.187403,0.125170
0.182423,0.123921
0.177970,0.122656
0.173953,0.121260
0.169803,0.126656
0.166618,0.128992
0.163731,0.131828
0.161454,0.137607
0.159581,0.144456
0.157042,0.150882
0.155171,0.143630
0.153273,0.154921
0.151461,0.157161
0.149960,0.154262
0.148440,0.150845
0.147440,0.155692
0.146115,0.149751
0.144820,0.150459
0.144126,0.151953
0.143013,0.155114
0.142135,0.153383
0.141049,0.153257
0.140095,0.150853
0.139223,0.152270
0.137852,0.152551
0.137248,0.151620
0.136402,0.157724
0.135382,0.145288
0.135152,0.155141
0.133801,0.153866
0.133549,0.146965
0.132705,0.151552
0.132508,0.150108
0.131746,0.152798
0.131689,0.156235
0.131181,0.150054
0.130527,0.146632
0.129803,0.149405
0.130142,0.152018
0.129316,0.151006
0.128503,0.147024
0.128348,0.147701
0.128131,0.148626
0.127634,0.148780
0.127457,0.148277
0.127174,0.153055
0.126866,0.149050
0.126420,0.147475
0.126556,0.145391
0.126105,0.147026
0.125903,0.148581
0.125398,0.146382
0.125721,0.149461
0.124978,0.147640
0.124920,0.149895
0.124918,0.148201
0.124637,0.147884
0.124806,0.148115
0.124578,0.149467
0.124190,0.148475
0.124211,0.148840
0.123999,0.146781
0.124028,0.146232
0.124022,0.148080
0.124262,0.147596
0.123896,0.148672
0.123966,0.147436
0.123760,0.148232
0.123702,0.147321
0.123687,0.146785
0.123694,0.147502
0.123555,0.147226
0.123227,0.147131
0.123599,0.147049
0.123860,0.147352
0.123818,0.146628
